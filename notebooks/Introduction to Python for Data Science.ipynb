{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Python for Data Science: Interactive Portion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're using a Jupyter Notebook here - but we could potentially be running this in IPython or nteract. Here are somet things to know about Python syntax. First off, commenting - comments begin with \"#\", or hashes, and docstrings begin with three quotation marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis is a docstring\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is a comment\n",
    "\"\"\"\n",
    "This is a docstring\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[We'll go through this tutorial](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html#training-a-classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is an import statement\n",
    "# what happens if i try them without install?\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "module"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# these are import statements\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "function"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(fetch_20newsgroups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "abc.ABCMeta"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(MultinomialNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_cache',\n",
       " '_abc_negative_cache',\n",
       " '_abc_negative_cache_version',\n",
       " '_abc_registry',\n",
       " '_count',\n",
       " '_estimator_type',\n",
       " '_get_coef',\n",
       " '_get_intercept',\n",
       " '_get_param_names',\n",
       " '_joint_log_likelihood',\n",
       " '_update_class_log_prior',\n",
       " '_update_feature_log_prob',\n",
       " 'coef_',\n",
       " 'fit',\n",
       " 'get_params',\n",
       " 'intercept_',\n",
       " 'partial_fit',\n",
       " 'predict',\n",
       " 'predict_log_proba',\n",
       " 'predict_proba',\n",
       " 'score',\n",
       " 'set_params']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(MultinomialNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the categories\n",
    "categories = ['alt.atheism', 'soc.religion.christian',\\\n",
    "        'comp.graphics', 'sci.med']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading dataset from http://people.csail.mit.edu/jrennie/20Newsgroups/20news-bydate.tar.gz (14 MB)\n"
     ]
    }
   ],
   "source": [
    "# sklearn object holder\n",
    "# twenty_train.data is a list of documents\n",
    "# so the tutorial told me to use this function, but I want to know what it does\n",
    "twenty_train = fetch_20newsgroups(subset = 'train',\n",
    "    categories=categories, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is when i'll import a helper module\n",
    "# i don't need to download this one because it \"comes with\" python\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([\"def fetch_20newsgroups(data_home=None, subset='train', categories=None,\\n\",\n",
       "  '                       shuffle=True, random_state=42,\\n',\n",
       "  '                       remove=(),\\n',\n",
       "  '                       download_if_missing=True):\\n',\n",
       "  '    \"\"\"Load the filenames and data from the 20 newsgroups dataset.\\n',\n",
       "  '\\n',\n",
       "  '    Read more in the :ref:`User Guide <20newsgroups>`.\\n',\n",
       "  '\\n',\n",
       "  '    Parameters\\n',\n",
       "  '    ----------\\n',\n",
       "  \"    subset : 'train' or 'test', 'all', optional\\n\",\n",
       "  \"        Select the dataset to load: 'train' for the training set, 'test'\\n\",\n",
       "  \"        for the test set, 'all' for both, with shuffled ordering.\\n\",\n",
       "  '\\n',\n",
       "  '    data_home : optional, default: None\\n',\n",
       "  '        Specify a download and cache folder for the datasets. If None,\\n',\n",
       "  \"        all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\\n\",\n",
       "  '\\n',\n",
       "  '    categories : None or collection of string or unicode\\n',\n",
       "  '        If None (default), load all the categories.\\n',\n",
       "  '        If not None, list of category names to load (other categories\\n',\n",
       "  '        ignored).\\n',\n",
       "  '\\n',\n",
       "  '    shuffle : bool, optional\\n',\n",
       "  '        Whether or not to shuffle the data: might be important for models that\\n',\n",
       "  '        make the assumption that the samples are independent and identically\\n',\n",
       "  '        distributed (i.i.d.), such as stochastic gradient descent.\\n',\n",
       "  '\\n',\n",
       "  '    random_state : numpy random number generator or seed integer\\n',\n",
       "  '        Used to shuffle the dataset.\\n',\n",
       "  '\\n',\n",
       "  '    download_if_missing : optional, True by default\\n',\n",
       "  '        If False, raise an IOError if the data is not locally available\\n',\n",
       "  '        instead of trying to download the data from the source site.\\n',\n",
       "  '\\n',\n",
       "  '    remove : tuple\\n',\n",
       "  \"        May contain any subset of ('headers', 'footers', 'quotes'). Each of\\n\",\n",
       "  '        these are kinds of text that will be detected and removed from the\\n',\n",
       "  '        newsgroup posts, preventing classifiers from overfitting on\\n',\n",
       "  '        metadata.\\n',\n",
       "  '\\n',\n",
       "  \"        'headers' removes newsgroup headers, 'footers' removes blocks at the\\n\",\n",
       "  \"        ends of posts that look like signatures, and 'quotes' removes lines\\n\",\n",
       "  '        that appear to be quoting another post.\\n',\n",
       "  '\\n',\n",
       "  \"        'headers' follows an exact standard; the other filters are not always\\n\",\n",
       "  '        correct.\\n',\n",
       "  '    \"\"\"\\n',\n",
       "  '\\n',\n",
       "  '    data_home = get_data_home(data_home=data_home)\\n',\n",
       "  '    cache_path = _pkl_filepath(data_home, CACHE_NAME)\\n',\n",
       "  '    twenty_home = os.path.join(data_home, \"20news_home\")\\n',\n",
       "  '    cache = None\\n',\n",
       "  '    if os.path.exists(cache_path):\\n',\n",
       "  '        try:\\n',\n",
       "  \"            with open(cache_path, 'rb') as f:\\n\",\n",
       "  '                compressed_content = f.read()\\n',\n",
       "  '            uncompressed_content = codecs.decode(\\n',\n",
       "  \"                compressed_content, 'zlib_codec')\\n\",\n",
       "  '            cache = pickle.loads(uncompressed_content)\\n',\n",
       "  '        except Exception as e:\\n',\n",
       "  \"            print(80 * '_')\\n\",\n",
       "  \"            print('Cache loading failed')\\n\",\n",
       "  \"            print(80 * '_')\\n\",\n",
       "  '            print(e)\\n',\n",
       "  '\\n',\n",
       "  '    if cache is None:\\n',\n",
       "  '        if download_if_missing:\\n',\n",
       "  '            logger.info(\"Downloading 20news dataset. \"\\n',\n",
       "  '                        \"This may take a few minutes.\")\\n',\n",
       "  '            cache = download_20newsgroups(target_dir=twenty_home,\\n',\n",
       "  '                                          cache_path=cache_path)\\n',\n",
       "  '        else:\\n',\n",
       "  \"            raise IOError('20Newsgroups dataset not found')\\n\",\n",
       "  '\\n',\n",
       "  \"    if subset in ('train', 'test'):\\n\",\n",
       "  '        data = cache[subset]\\n',\n",
       "  \"    elif subset == 'all':\\n\",\n",
       "  '        data_lst = list()\\n',\n",
       "  '        target = list()\\n',\n",
       "  '        filenames = list()\\n',\n",
       "  \"        for subset in ('train', 'test'):\\n\",\n",
       "  '            data = cache[subset]\\n',\n",
       "  '            data_lst.extend(data.data)\\n',\n",
       "  '            target.extend(data.target)\\n',\n",
       "  '            filenames.extend(data.filenames)\\n',\n",
       "  '\\n',\n",
       "  '        data.data = data_lst\\n',\n",
       "  '        data.target = np.array(target)\\n',\n",
       "  '        data.filenames = np.array(filenames)\\n',\n",
       "  '    else:\\n',\n",
       "  '        raise ValueError(\\n',\n",
       "  '            \"subset can only be \\'train\\', \\'test\\' or \\'all\\', got \\'%s\\'\" % subset)\\n',\n",
       "  '\\n',\n",
       "  \"    data.description = 'the 20 newsgroups by date dataset'\\n\",\n",
       "  '\\n',\n",
       "  \"    if 'headers' in remove:\\n\",\n",
       "  '        data.data = [strip_newsgroup_header(text) for text in data.data]\\n',\n",
       "  \"    if 'footers' in remove:\\n\",\n",
       "  '        data.data = [strip_newsgroup_footer(text) for text in data.data]\\n',\n",
       "  \"    if 'quotes' in remove:\\n\",\n",
       "  '        data.data = [strip_newsgroup_quoting(text) for text in data.data]\\n',\n",
       "  '\\n',\n",
       "  '    if categories is not None:\\n',\n",
       "  '        labels = [(data.target_names.index(cat), cat) for cat in categories]\\n',\n",
       "  '        # Sort the categories to have the ordering of the labels\\n',\n",
       "  '        labels.sort()\\n',\n",
       "  '        labels, categories = zip(*labels)\\n',\n",
       "  '        mask = np.in1d(data.target, labels)\\n',\n",
       "  '        data.filenames = data.filenames[mask]\\n',\n",
       "  '        data.target = data.target[mask]\\n',\n",
       "  '        # searchsorted to have continuous labels\\n',\n",
       "  '        data.target = np.searchsorted(labels, data.target)\\n',\n",
       "  '        data.target_names = list(categories)\\n',\n",
       "  '        # Use an object array to shuffle: avoids memory copy\\n',\n",
       "  '        data_lst = np.array(data.data, dtype=object)\\n',\n",
       "  '        data_lst = data_lst[mask]\\n',\n",
       "  '        data.data = data_lst.tolist()\\n',\n",
       "  '\\n',\n",
       "  '    if shuffle:\\n',\n",
       "  '        random_state = check_random_state(random_state)\\n',\n",
       "  '        indices = np.arange(data.target.shape[0])\\n',\n",
       "  '        random_state.shuffle(indices)\\n',\n",
       "  '        data.filenames = data.filenames[indices]\\n',\n",
       "  '        data.target = data.target[indices]\\n',\n",
       "  '        # Use an object array to shuffle: avoids memory copy\\n',\n",
       "  '        data_lst = np.array(data.data, dtype=object)\\n',\n",
       "  '        data_lst = data_lst[indices]\\n',\n",
       "  '        data.data = data_lst.tolist()\\n',\n",
       "  '\\n',\n",
       "  '    return data\\n'],\n",
       " 154)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.getsourcelines(fetch_20newsgroups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.datasets.base.Bunch"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(twenty_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DESCR', 'data', 'description', 'filenames', 'target', 'target_names']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(twenty_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "twenty_train.target is an array that contains\n",
    "the document \"categories\" in the same order as data\n",
    "twenty_train.data is an array that contains a list of\n",
    "documents\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.feature_extraction.text.CountVectorizer"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so what we did here is that we \n",
    "type(count_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "the fit_transform method takes in the raw documents\n",
    "and returns a term-document matrix with the shape\n",
    "[n_samples, n_features]\n",
    "\"\"\"\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the tutorial tells me to do this\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntwenty_train.target is an array that contains\\nthe document \"categories\" in the same order as data\\ntwenty_train.data is an array that contains a list of\\ndocuments\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# twenty_train.data is a list of documents\n",
    "twenty_train = fetch_20newsgroups(subset = 'train',\n",
    "        categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "\"\"\"\n",
    "twenty_train.target is an array that contains\n",
    "the document \"categories\" in the same order as data\n",
    "twenty_train.data is an array that contains a list of\n",
    "documents\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_counts = count_vect.fit_transform(twenty_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# okay, but where does count_vect come from?\n",
    "# If you scroll up, above, it actually comes from the line\n",
    "# count_vect = CountVectorizer() - when we make an INSTANCE of the CLASS CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use the TfidfTransformer to create term_frequencies\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This creates term-frequency sparse matrices for each document\n",
    "X_train_tf[0] is the sparse matrix for document 0\n",
    "\"\"\"\n",
    "\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x35788 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 73 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we're making a new instance of TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "# so we're fitting it\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clf is the classifier\n",
    "# MultinomialNB is the naive bayes classifier\n",
    "clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define some new documents\n",
    "docs_new = ['GPU and graphics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make a term-doc matrix\n",
    "\"\"\"\n",
    "We use transform instead of fit-transform because\n",
    "the transformer is already fit to the training data\n",
    "\"\"\"\n",
    "X_new_counts = count_vect.transform(docs_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make a tf-idf sparse matrix\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted = clf.predict(X_new_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'God is love' => soc.religion.christian\n",
      "'GPU and graphics' => comp.graphics\n"
     ]
    }
   ],
   "source": [
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('%r => %s' % (doc, twenty_train.target_names[category]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
